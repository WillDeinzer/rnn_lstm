{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d3add0",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e126b6",
   "metadata": {},
   "source": [
    "### LSTM Gates\n",
    "\n",
    "* Input Gate - decides how much new information to keep in the cell state - uses sigmoid, tanh\n",
    "\n",
    "* Forget Gate - determines how much old information to remove from cell state - employs a sigmoid function\n",
    "\n",
    "* Output Gate - manages what the next hidden layer / output will receive from the cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d49fa",
   "metadata": {},
   "source": [
    "### Managing Information Flow\n",
    "\n",
    "* Each gate can either add or remove information from the cell state - tune system to properly handle long-term dependencies\n",
    "* Sigmoids ensure that updates are precise and controlled - good for addressing vanishing gradient problem\n",
    "* Helps learn and remember info across much longer sequences than traditional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424a7c0",
   "metadata": {},
   "source": [
    "### Forget Gate\n",
    "\n",
    "$ f_t = \\sigma (W_f * [h_{t - 1}, x_t] + b_f) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fbddf",
   "metadata": {},
   "source": [
    "### Input Gate, Candidate Cell State\n",
    "\n",
    "$ i_t = \\sigma (W_i * [h_{t - 1}, x_t] + b_i) $\n",
    "\n",
    "$ \\tilde{C_t} = tanh(W_C * [h_{t - 1}, x_t] + b_C) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddea789",
   "metadata": {},
   "source": [
    "### Cell State Update\n",
    "\n",
    "$ C_t = f_t * C_{t - 1} + i_t * \\tilde{C_t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85356830",
   "metadata": {},
   "source": [
    "### Output Gate\n",
    "\n",
    "$ o_t = \\sigma (W_o * [h_{t - 1}, x_t] + b_o) $\n",
    "\n",
    "$ h_t = o_t * tanh(C_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1105d0d2",
   "metadata": {},
   "source": [
    "### Gating Mechanisms\n",
    "\n",
    "* Designed to combat vanishing gradient problem - control the flow of gradients during backprop\n",
    "\n",
    "* Sigmoid function acts as a gate, tanh helps keep network's activations normalized - leads to faster convergence\n",
    "\n",
    "* Ensures that important info is retained over longer sequences, enhancing memory capabilities"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
